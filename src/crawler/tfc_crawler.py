# src/crawler/tfc_crawler.py

import requests                      # ç™¼é€ HTTP è«‹æ±‚
from bs4 import BeautifulSoup       # è§£æ HTML çµæ§‹
import csv                          # è¼¸å‡ºç‚º CSV
import os                           # å»ºç«‹è³‡æ–™å¤¾
import time                         # è¨­å®š delayï¼Œé¿å…è«‹æ±‚éå¿«è¢«å°é–

# ç¶²ç«™ä¸»ç¶²å€
BASE_URL = "https://tfc-taiwan.org.tw"

# æœ€æ–°æ¶ˆæ¯é é¢ URL æ¨¡æ¿ï¼ˆå¯ç”¨ page=0, 1, 2... ä¾åºç¿»é ï¼‰
LATEST_NEWS_URL = BASE_URL + "/latest-news?page={}"

# è¼¸å‡ºçš„ CSV æª”æ¡ˆè·¯å¾‘
OUTPUT_FILE = "data/raw/tfc_articles.csv"

# åŠ å…¥æ¨™é ­ä¾†é¿å…è¢«ç¶²ç«™æ‹’çµ•
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/117.0.0.0 Safari/537.36"
}

# ---------------------------------------------------------
# å–å¾—å–®ä¸€é é¢ä¸Šçš„æŸ¥æ ¸æ–‡ç« é€£çµï¼ˆåªæŠ“ /fact-check-reports/ çš„æ–‡ç« ï¼‰
def fetch_article_links(page_num):
    url = LATEST_NEWS_URL.format(page_num)
    res = requests.get(url, headers=HEADERS)
    if res.status_code != 200:
        print(f"âŒ Failed to fetch page {page_num}")
        return []

    soup = BeautifulSoup(res.text, 'html.parser')

    # æŠ“ figure å€å¡Šä¸­çš„ <a> æ¨™ç±¤ï¼ˆç”¨æ–¼é€£çµæŸ¥æ ¸æ–‡ç« ï¼‰
    figure_links = soup.select('figure.wp-block-kadence-image a')

    links = []
    for a_tag in figure_links:
        href = a_tag.get('href')
        if href and '/fact-check-reports/' in href:
            if href.startswith("http"):
                links.append(href)
            else:
                links.append(BASE_URL + href)
    print(f"âœ… Page {page_num}: found {len(links)} article links")
    return links


# ---------------------------------------------------------
# æ‹¿åˆ°å–®ä¸€ç¯‡æ–‡ç« çš„è©³ç´°è³‡æ–™ï¼ˆæ¨™é¡Œã€æ—¥æœŸã€æŸ¥æ ¸çµæœã€å…§å®¹ï¼‰
def fetch_article_detail(url):
    res = requests.get(url, headers=HEADERS)  # âœ… åŒæ¨£åŠ ä¸Š headers
    if res.status_code != 200:
        print(f"âŒ Failed to fetch article {url}")
        return None

    soup = BeautifulSoup(res.text, 'html.parser')

    try:
        # æ¨™é¡Œ
        title = soup.find('h1', class_='article-title').text.strip()

        # ç™¼å¸ƒæ—¥æœŸ
        date = soup.find('time').text.strip()

        # æŸ¥æ ¸çµè«–ï¼ˆä¾‹å¦‚ã€ŒéŒ¯èª¤ã€ã€ã€Œéƒ¨ä»½éŒ¯èª¤ã€ã€ã€Œç„¡æ˜ç¢ºæ ¹æ“šã€ï¼‰
        verdict_elem = soup.select_one('.field--name-field-verdict .field__item')
        verdict = verdict_elem.text.strip() if verdict_elem else 'æœªçŸ¥'

        # æ–‡ç« å…§æ–‡ï¼ˆHTML ä¸­çš„ä¸»é«”æ–‡å­—å€å¡Šï¼‰
        content_block = soup.select_one('.field--name-body')
        content = content_block.get_text(strip=True) if content_block else ""
    except Exception as e:
        print(f"âš ï¸ Error parsing {url}: {e}")
        return None

    return {
        'title': title,
        'date': date,
        'verdict': verdict,
        'url': url,
        'content': content[:300]  # åªå–å‰ 300 å­—åšæ‘˜è¦ç”¨
    }

# ---------------------------------------------------------
# å°‡æ‰€æœ‰æ–‡ç« å¯«å…¥ CSV æª”æ¡ˆ
def save_to_csv(articles, filename):
    # è‹¥ç›®éŒ„ä¸å­˜åœ¨å‰‡è‡ªå‹•å»ºç«‹
    os.makedirs(os.path.dirname(filename), exist_ok=True)

    # é–‹å•Ÿ CSV æª”æ¡ˆä¸¦å¯«å…¥æ¬„ä½èˆ‡æ¯ç¯‡æ–‡ç« è³‡æ–™
    with open(filename, mode='w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=['title', 'date', 'verdict', 'url', 'content'])
        writer.writeheader()
        for article in articles:
            writer.writerow(article)
    print(f"âœ… Saved {len(articles)} articles to {filename}")

# ---------------------------------------------------------
# ä¸»æµç¨‹ï¼šçˆ¬å–å‰ N é ï¼Œçµ„åˆæˆä¸€å€‹è³‡æ–™é›†ä¸¦å­˜æª”
def main(max_pages=3):
    all_articles = []
    for page in range(1, max_pages+1):  # æ³¨æ„ï¼šTFC çš„é ç¢¼å¾ 1 é–‹å§‹
        print(f"ğŸ” Fetching page {page}...")
        links = fetch_article_links(page)
        for link in links:
            detail = fetch_article_detail(link)
            if detail:
                all_articles.append(detail)
            time.sleep(0.5)  # ç¦®è²Œæ€§å»¶é²ï¼Œé¿å…è«‹æ±‚éå¿«
    save_to_csv(all_articles, OUTPUT_FILE)

# ---------------------------------------------------------
# ç¨‹å¼é€²å…¥é»
if __name__ == "__main__":
    main(max_pages=3)  # é è¨­åªæŠ“å‰ 3 é ï¼Œå¯è‡ªè¡Œå¢åŠ é æ•¸
