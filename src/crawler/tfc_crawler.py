# src/crawler/tfc_crawler.py

import requests                      # ç™¼é€ HTTP è«‹æ±‚
from bs4 import BeautifulSoup       # è§£æ HTML çµæ§‹
import csv                          # å„²å‡ºç‚º CSV æª”
import os                           # è™•ç†è·¯å¾‘èˆ‡å»ºç«‹è³‡æ–™å¤¾
import time                         # åŠ å…¥å»¶é²ï¼Œé¿å…å°ç¶²ç«™é€ æˆå£“åŠ›
import re                           # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æ“·å–æ—¥æœŸèˆ‡æ¨™é¡Œçµæ§‹

# ç¶²ç«™ä¸»ç¶²å€
BASE_URL = "https://tfc-taiwan.org.tw"

# è¼¸å‡ºçš„ CSV æª”æ¡ˆå„²å­˜è·¯å¾‘
OUTPUT_FILE = "data/raw/tfc_articles.csv"

# åŠ å…¥æ¨™é ­æ¨¡æ“¬ç€è¦½å™¨ï¼Œä»¥å…è¢«ç¶²ç«™é˜»æ“‹æˆ–å›å‚³éŒ¯èª¤æ ¼å¼
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/117.0.0.0 Safari/537.36"
}

# ---------------------------------------------------------
# å‡½å¼ï¼šæŠ“å–æŸä¸€é çš„æ‰€æœ‰æŸ¥æ ¸æ–‡ç« é€£çµ
def fetch_article_links(page_num):
    # ç¬¬ä¸€é ç¶²å€æ²’æœ‰åƒæ•¸ï¼Œä¹‹å¾Œé æ•¸ä½¿ç”¨ ?pg=
    if page_num == 1:
        url = BASE_URL + "/latest-news/"
    else:
        url = BASE_URL + f"/latest-news/?pg={page_num}"

    # ç™¼é€ HTTP è«‹æ±‚
    res = requests.get(url, headers=HEADERS)
    if res.status_code != 200:
        print(f"âŒ Failed to fetch page {page_num}")
        return []

    # ä½¿ç”¨ BeautifulSoup è§£æ HTML çµæ§‹
    soup = BeautifulSoup(res.text, 'html.parser')

    # é¸å–æ‰€æœ‰å«æœ‰ "Read More" æŒ‰éˆ•çš„é€£çµ
    link_tags = soup.select('a.kb-button')

    links = []
    for a_tag in link_tags:
        href = a_tag.get('href')
        if href and '/fact-check-reports/' in href:
            # å¦‚æœä¸æ˜¯å®Œæ•´ URLï¼ŒåŠ ä¸Šä¸»æ©Ÿåç¨±è£œè¶³
            full_url = href if href.startswith('http') else BASE_URL + href
            # é¿å…é‡è¤‡åŠ å…¥ç›¸åŒé€£çµ
            if full_url not in links:
                links.append(full_url)

    print(f"âœ… Page {page_num}: found {len(links)} article links")
    return links

# ---------------------------------------------------------
# å‡½å¼ï¼šæ“·å–å–®ç¯‡æŸ¥æ ¸æ–‡ç« çš„è©³ç´°å…§å®¹
def fetch_article_detail(url):
    res = requests.get(url, headers=HEADERS)
    if res.status_code != 200:
        print(f"âŒ Failed to fetch article {url}")
        return None

    soup = BeautifulSoup(res.text, 'html.parser')

    try:
        # åˆ¤æ–·æ˜¯å¦ç‚ºèˆŠç‰ˆçµæ§‹ï¼šèˆŠç‰ˆå«æœ‰ <h1 class="entry-title">
        is_old = bool(soup.select_one('h1.entry-title'))

        # ---------------- æ–°ç‰ˆçµæ§‹ ----------------
        if not is_old:
            # æ–°ç‰ˆæ¨™é¡Œåœ¨ p > strong ä¸­
            title_tag = soup.select_one('p.wp-block-kadence-advancedheading strong')
            title = title_tag.text.strip() if title_tag else 'æœªçŸ¥'

            # æ–°ç‰ˆæ—¥æœŸå¾ <strong> æ¨™ç±¤ä¸­æœå°‹åŒ…å«ã€Œç™¼ä½ˆã€çš„æ®µè½
            date = 'æœªçŸ¥'
            for tag in soup.find_all('strong'):
                if 'ç™¼ä½ˆ' in tag.text:
                    parent = tag.parent
                    if parent:
                        text = parent.get_text(strip=True).replace("ç™¼ä½ˆï¼š", "").replace("ç™¼ä½ˆ", "").strip()
                        if re.match(r"\d{4}-\d{2}-\d{2}", text):  # ç¢ºä¿æ˜¯æ­£ç¢ºæ ¼å¼
                            date = text
                    break

            # æŸ¥æ ¸çµè«–ï¼šå¾ verdict å…ƒç´ å–å¾—ï¼ˆå¦‚ã€ŒéŒ¯èª¤ã€ã€ã€Œæ­£ç¢ºã€ï¼‰
            verdict_tag = soup.select_one('a.kb-dynamic-list-item-link')
            verdict = verdict_tag.text.strip() if verdict_tag else 'æœªçŸ¥'

            # å…§æ–‡ï¼šå¾å½©è‰²èƒŒæ™¯æ®µè½ä¸­é¸å–æ‰€æœ‰ p
            paragraphs = soup.select('p.wp-block-kadence-advancedheading.has-theme-palette-7-background-color')
            content = '\n'.join([p.text.strip() for p in paragraphs])
            content = content[:100]

        # ---------------- èˆŠç‰ˆçµæ§‹ ----------------
        else:
            # æ¨™é¡Œèˆ‡æŸ¥æ ¸çµæœå¯«åœ¨ h1 ä¸­ï¼Œä¾‹å¦‚ï¼šã€éŒ¯èª¤ã€‘é¦™è•‰è‡´ç™Œ
            title_h1 = soup.select_one('h1.entry-title')
            full_title = title_h1.text.strip() if title_h1 else 'æœªçŸ¥'

            # ç”¨æ­£è¦è¡¨ç¤ºæ³•æ“·å–ã€æŸ¥æ ¸çµè«–ã€‘
            verdict_match = re.match(r'ã€(.+?)ã€‘', full_title)
            verdict = verdict_match.group(1) if verdict_match else 'æœªçŸ¥'
            title = full_title.replace(f'ã€{verdict}ã€‘', '').strip()

            # æ—¥æœŸï¼šå¾ <div class="entity-list-date"> ä¸­æ“·å–ç™¼å¸ƒæ—¥æœŸ
            date_tag = soup.select_one('div.entity-list-date')
            date = 'æœªçŸ¥'
            if date_tag and "ç™¼å¸ƒæ—¥æœŸ" in date_tag.text:
                match = re.search(r'\d{4}-\d{2}-\d{2}', date_tag.text)
                if match:
                    date = match.group(0)

            # å…§æ–‡ï¼šæŠ“å–æ‰€æœ‰ <p> æ®µè½
            paragraphs = soup.select('div.entry-content.single-content p')
            content = '\n'.join([p.text.strip() for p in paragraphs if p.text.strip()])
            content = content[:100]

        # å›å‚³è©²ç¯‡æ–‡ç« çš„æ‰€æœ‰æ¬„ä½è³‡è¨Š
        return {
            'title': title,
            'date': date,
            'verdict': verdict,
            'url': url,
            'content': content
        }

    except Exception as e:
        print(f"âš ï¸ Error parsing {url}: {e}")
        return None

# ---------------------------------------------------------
# å‡½å¼ï¼šå°‡çˆ¬ä¸‹ä¾†çš„æ‰€æœ‰æ–‡ç« å¯«å…¥ CSV
def save_to_csv(articles, filename):
    os.makedirs(os.path.dirname(filename), exist_ok=True)  # è‹¥è³‡æ–™å¤¾ä¸å­˜åœ¨å°±å»ºç«‹
    with open(filename, mode='w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=['title', 'date', 'verdict', 'url', 'content'])
        writer.writeheader()
        for article in articles:
            writer.writerow(article)
    print(f"âœ… Saved {len(articles)} articles to {filename}")

# ---------------------------------------------------------
# ä¸»å‡½å¼ï¼šä¾åºæŠ“å–æ¯ä¸€é çš„æ‰€æœ‰æ–‡ç« ä¸¦å„²å­˜
def main(max_pages):
    all_articles = []
    for page in range(1, max_pages + 1):
        print(f"ğŸ” Fetching page {page}...")
        links = fetch_article_links(page)
        for link in links:
            detail = fetch_article_detail(link)
            if detail:
                all_articles.append(detail)
            time.sleep(0.5)  # æ¯ç¯‡æ–‡ç« å»¶é² 0.5 ç§’ï¼Œé¿å…å°ç¶²ç«™é€ æˆéå¤šè«‹æ±‚
    save_to_csv(all_articles, OUTPUT_FILE)

# ---------------------------------------------------------
# ç¨‹å¼é€²å…¥é»ï¼ˆmainï¼‰
if __name__ == "__main__":
    main(max_pages=605)  # é è¨­æŠ“å–å‰ 605 é æ–‡ç« ï¼ˆå¯ä¾éœ€æ±‚èª¿æ•´ï¼‰
